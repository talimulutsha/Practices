{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "In machine learning, a pipeline refers to a series of data processing steps that are applied sequentially to a dataset to transform and analyze it. These steps typically include data preprocessing, feature extraction, feature selection, model training, and model evaluation. Pipelines are essential for streamlining the machine learning workflow, ensuring consistency, and enabling efficient experimentation and deployment.\n",
    "\n",
    "Here's a breakdown of the typical components of a machine learning pipeline:\n",
    "\n",
    "1. **Data Preprocessing**: This step involves cleaning and preparing the raw data for analysis. It may include tasks such as handling missing values, removing outliers, scaling features, and encoding categorical variables.\n",
    "\n",
    "2. **Feature Extraction/Transformation**: In this step, relevant features are extracted from the preprocessed data or new features are created through transformations. Feature extraction techniques may include dimensionality reduction methods like Principal Component Analysis (PCA) or feature engineering to create new features from existing ones.\n",
    "\n",
    "3. **Feature Selection**: Sometimes, not all features are relevant for the model or may even introduce noise. Feature selection techniques are used to choose the most relevant features for training the model, improving model performance and reducing overfitting.\n",
    "\n",
    "4. **Model Training**: This step involves selecting an appropriate machine learning algorithm and training it on the processed data. The choice of algorithm depends on the nature of the problem (classification, regression, clustering, etc.) and the characteristics of the data.\n",
    "\n",
    "5. **Model Evaluation**: After training the model, it is evaluated using evaluation metrics appropriate for the task (e.g., accuracy, precision, recall, F1-score for classification; RMSE, MAE for regression). The model's performance is assessed on a separate test dataset to estimate its generalization ability.\n",
    "\n",
    "6. **Hyperparameter Tuning**: Many machine learning algorithms have hyperparameters that need to be tuned to optimize model performance. Hyperparameter tuning involves selecting the best set of hyperparameters through techniques like grid search, random search, or Bayesian optimization.\n",
    "\n",
    "7. **Model Deployment**: Once a satisfactory model is trained and evaluated, it can be deployed to make predictions on new, unseen data. Deployment involves integrating the model into a production environment where it can receive input data and provide predictions in real-time.\n",
    "\n",
    "By organizing the machine learning workflow into a pipeline, it becomes easier to iterate through different combinations of preprocessing techniques, feature sets, algorithms, and hyperparameters to find the best model for the problem at hand. Additionally, pipelines facilitate reproducibility and scalability, making it easier to maintain and update machine learning systems over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7877094972067039\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# load the Titanic dataset from seaborn\n",
    "titanic_data= sns.load_dataset('titanic')\n",
    "\n",
    "# select feature and target variables\n",
    "X = titanic_data[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y = titanic_data['survived']\n",
    "\n",
    "# split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# define the column transformer\n",
    "numeric_feature= ['age', 'fare']\n",
    "catagorical_feature = ['pclass', 'sex', 'embarked']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "catagorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_feature),\n",
    "    ('cat', catagorical_transformer, catagorical_feature)\n",
    "])\n",
    "\n",
    "\n",
    "#Create a pipeline with the preprocessor and RandomForestClassifier\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline on hyper perameter tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8212290502793296\n",
      "Best Hyperparameters: {'model__max_depth': 30, 'model__min_samples_split': 5, 'model__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# load the Titanic dataset from seaborn\n",
    "titanic_data= sns.load_dataset('titanic')\n",
    "\n",
    "# select feature and target variables\n",
    "X = titanic_data[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y = titanic_data['survived']\n",
    "\n",
    "# split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline=Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define the hyperparameters to tune\n",
    "hyperparameters = {\n",
    "    'model__n_estimators': [100, 200, 300, 500],\n",
    "    'model__max_depth': [None, 5, 10,30],\n",
    "    'model__min_samples_split': [2, 5, 10, 15]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# perform grid search cross-validation\n",
    "grid_search= GridSearchCV(pipeline, hyperparameters, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# make predictions on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "# evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
